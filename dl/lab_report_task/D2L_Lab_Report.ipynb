{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7da0d75-0a97-4ed9-94fe-112d84f19698",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning for MSc - Lab Coding Task 2025\n",
    "\n",
    "## Preliminary Work\n",
    "\n",
    "You should have already done Lab 2 ensuring that you read through the actual Python code and understand how the code is working.\n",
    "\n",
    "You should have also read the start of Chapter 3 of the Dive into Deep Learning (D2L) Book and understood this.\n",
    "\n",
    "This task has two main Intended Learning Outcomes (ILOs):\n",
    "\n",
    "1. You should be able to look at Python code similar to Lab 2 Task 1 and understand what all the parts of the code are doing.\n",
    "2. You should gain an understanding of the structure of the Deep Learning Framework in the D2L Book (and hence the process of doing Deep Learning). This is vital to understand material in later chapters of the book. (Understanding the initial framework is quite difficult but it becomes much easier once it is repeated for different types of deep learning networks.)\n",
    "\n",
    "## Overall Objective\n",
    "\n",
    "The notebook below consists of notebook code from Chapter 3 of the D2L book (slightly modified) that synthesizes linear regression data with Gaussian noise and then fits a linear model to it. I have also added the plotting code from Lab 2 so that you can visualize the output of the linear model in terms of original training data, ground-truth function and the function the model learned.\n",
    "\n",
    "Your overall objective is to change this code so it does the Task 1 in Lab 2.\n",
    "\n",
    "The final graphical output of the code should look very similar to the output given on Moodle.  \n",
    "\n",
    "You should then submit the notebook to the Moodle submission site.\n",
    "\n",
    "This is an individual coursework so you should do it by yourself (without Generative AI or the help of other students!)\n",
    "\n",
    "If you don't do this coursework by yourself then you will not learn waht is needed, you will find all the subsequent studies harder and it will ultimately affect your overall performance in the final exam.\n",
    "\n",
    "## Key Tasks\n",
    "\n",
    "1. Ensure you have done the prerequisites preliminary work given above.\n",
    "2. Slowly run the notebook cells while you read the notebook cell descriptions and code. Make sure you understand how the code is working and the structure of the framework.\n",
    "3. Modify the code to generate / synthesize appropriate data as given in Lab 2, Task 1. (Including changing the code to use the same probability distributions as Lab 2 / Task 1.)\n",
    "4. Modify the code to use the more complicated model given in Lab 2, Task 1. (Hint: Look at D2L Book Chapter 5 to see how this could be done.)\n",
    "5. **You should modify the template to make your data synthesis, neural network architecture and optimization approach as close to the original Task 1 Lab 2 as possible except: generate 500 training points and 500 validation points, use a batch size of 100 and run 100 epochs.**\n",
    "6. Look at the results and resolve any issues that might be present. (Some aspects of this task require problem solving and you will need to solve any issues that may arise - potentially including coding issues with this original template code!)\n",
    "7. Ensure that you have \"clean code\" and \"notebook documentation\" - you should remove anything that is not relevant to the new model - and you should update Markdown documentation to reflect how the new code works.\n",
    "8. **IMPORTANT - You should not change the plotting code !**\n",
    "9. Upload the notebook of your solution to Moodle.\n",
    "\n",
    "Good Luck !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c9199",
   "metadata": {},
   "source": [
    "\n",
    "# Synthetic Regression Data (Slightly modified from D2L Book Section 3.3)\n",
    ":label:`sec_synthetic-regression-data`\n",
    "\n",
    "\n",
    "Machine learning is all about extracting information from data.\n",
    "So you might wonder, what could we possibly learn from synthetic data?\n",
    "While we might not care intrinsically about the patterns \n",
    "that we ourselves baked into an artificial data generating model,\n",
    "such datasets are nevertheless useful for didactic purposes,\n",
    "helping us to evaluate the properties of our learning \n",
    "algorithms and to confirm that our implementations work as expected.\n",
    "For example, if we create data for which the correct parameters are known *a priori*,\n",
    "then we can check that our model can in fact recover them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaed54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29299f8c",
   "metadata": {},
   "source": [
    "## Generating the Dataset\n",
    "\n",
    "For this example, we will work in low dimension\n",
    "for succinctness.\n",
    "The following code snippet generates 1000 examples\n",
    "with 1-dimensional features drawn \n",
    "from a standard normal distribution.\n",
    "The resulting design matrix $\\mathbf{X}$\n",
    "belongs to $\\mathbb{R}^{1000 \\times 1}$. \n",
    "We generate each label by applying \n",
    "a *ground truth* linear function, \n",
    "corrupting them via additive noise $\\boldsymbol{\\epsilon}$, \n",
    "drawn independently and identically for each example:\n",
    "\n",
    "(**$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\boldsymbol{\\epsilon}.$$**)\n",
    "\n",
    "For convenience we assume that $\\boldsymbol{\\epsilon}$ is drawn \n",
    "from a standard normal distribution with mean $\\mu= 0$ \n",
    "and standard deviation $\\sigma = 1.0$.\n",
    "Note that for object-oriented design\n",
    "we add the code to the `__init__` method of a subclass of `d2l.DataModule` (introduced in :numref:`oo-design-data`). \n",
    "It is good practice to allow the setting of any additional hyperparameters. \n",
    "We accomplish this with `save_hyperparameters()`. \n",
    "The `batch_size` will be determined later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22396da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData(d2l.DataModule):  #@save\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=1.0, num_train=100, num_val=100,\n",
    "                 batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        epsilon = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + epsilon\n",
    "\n",
    "    def ground_truth_function(self,x):\n",
    "        return self.w * x + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55aaf8",
   "metadata": {},
   "source": [
    "Below, we set the true parameters to $\\mathbf{w} = [2.0,]$ and $b = 4.2$.\n",
    "Later, we can check our estimated parameters against these *ground truth* values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d9da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SyntheticRegressionData(w=torch.tensor([2.0,]), b=4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b685fe",
   "metadata": {},
   "source": [
    "[**Each row in `features` consists of a single scalar value (in the original it was a vector in $\\mathbb{R}^2$) and each row in `labels` is a scalar.**]\n",
    "\n",
    "Let's have a look at the first entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330c3f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([0.2843]) \n",
      "label: tensor([5.4883])\n"
     ]
    }
   ],
   "source": [
    "print('features:', data.X[0],'\\nlabel:', data.y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d014c1",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Training machine learning models often requires multiple passes over a dataset, \n",
    "grabbing one minibatch of examples at a time. \n",
    "This data is then used to update the model. \n",
    "To illustrate how this works, we \n",
    "[**implement the `get_dataloader` method,**] \n",
    "registering it in the `SyntheticRegressionData` class via `add_to_class` (introduced in :numref:`oo-design-utilities`).\n",
    "It (**takes a batch size, a matrix of features,\n",
    "and a vector of labels, and generates minibatches of size `batch_size`.**)\n",
    "As such, each minibatch consists of a tuple of features and labels. \n",
    "Note that we need to be mindful of whether we're in training or validation mode: \n",
    "in the former, we will want to read the data in random order, \n",
    "whereas for the latter, being able to read data in a pre-defined order \n",
    "may be important for debugging purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39329929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the concise implementation of the data loader given below.\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train))\n",
    "        # The examples are read in random order\n",
    "        random.shuffle(indices)\n",
    "    else:\n",
    "        indices = list(range(self.num_train, self.num_train+self.num_val))\n",
    "    for i in range(0, len(indices), self.batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: i+self.batch_size])\n",
    "        yield self.X[batch_indices], self.y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70915d16",
   "metadata": {},
   "source": [
    "To build some intuition, let's inspect the first minibatch of\n",
    "data. Each minibatch of features provides us with both its size and the dimensionality of input features.\n",
    "Likewise, our minibatch of labels will have a matching shape given by `batch_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071bfb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 1]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ef934",
   "metadata": {},
   "source": [
    "While seemingly innocuous, the invocation \n",
    "of `iter(data.train_dataloader())` \n",
    "illustrates the power of Python's object-oriented design. \n",
    "Note that we added a method to the `SyntheticRegressionData` class\n",
    "*after* creating the `data` object. \n",
    "Nonetheless, the object benefits from \n",
    "the *ex post facto* addition of functionality to the class.\n",
    "\n",
    "Throughout the iteration we obtain distinct minibatches\n",
    "until the entire dataset has been exhausted (try this).\n",
    "While the iteration implemented above is good for didactic purposes,\n",
    "it is inefficient in ways that might get us into trouble with real problems.\n",
    "For example, it requires that we load all the data in memory\n",
    "and that we perform lots of random memory access.\n",
    "The built-in iterators implemented in a deep learning framework\n",
    "are considerably more efficient and they can deal\n",
    "with sources such as data stored in files, \n",
    "data received via a stream, \n",
    "and data generated or processed on the fly. \n",
    "Next let's try to implement the same method using built-in iterators.\n",
    "\n",
    "## Concise Implementation of the Data Loader\n",
    "\n",
    "Rather than writing our own iterator,\n",
    "we can [**call the existing API in a framework to load data.**]\n",
    "As before, we need a dataset with features `X` and labels `y`. \n",
    "Beyond that, we set `batch_size` in the built-in data loader \n",
    "and let it take care of shuffling examples  efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e157003",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                       shuffle=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1827c",
   "metadata": {},
   "source": [
    "The new data loader behaves just like the previous one, except that it is more efficient and has some added functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d82000",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2230f9",
   "metadata": {},
   "source": [
    "For instance, the data loader provided by the framework API \n",
    "supports the built-in `__len__` method, \n",
    "so we can query its length, \n",
    "i.e., the number of batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6551e8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data loaders are a convenient way of abstracting out \n",
    "the process of loading and manipulating data. \n",
    "This way the same machine learning *algorithm* \n",
    "is capable of processing many different types and sources of data \n",
    "without the need for modification. \n",
    "One of the nice things about data loaders \n",
    "is that they can be composed. \n",
    "For instance, we might be loading images \n",
    "and then have a postprocessing filter \n",
    "that crops them or modifies them in other ways. \n",
    "As such, data loaders can be used \n",
    "to describe an entire data processing pipeline. \n",
    "\n",
    "As for the model itself, the two-dimensional linear model \n",
    "is about the simplest we might encounter. \n",
    "It lets us test out the accuracy of regression models \n",
    "without worrying about having insufficient amounts of data \n",
    "or an underdetermined system of equations. \n",
    "We will put this to good use in the next section.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c1874",
   "metadata": {},
   "source": [
    "# Concise Implementation of Linear Regression (From D2L Book Section 3.5)\n",
    ":label:`sec_linear_concise`\n",
    "\n",
    "Deep learning has witnessed a sort of Cambrian explosion\n",
    "over the past decade.\n",
    "The sheer number of techniques, applications and algorithms by far surpasses the\n",
    "progress of previous decades. \n",
    "This is due to a fortuitous combination of multiple factors,\n",
    "one of which is the powerful free tools\n",
    "offered by a number of open-source deep learning frameworks.\n",
    "Theano :cite:`Bergstra.Breuleux.Bastien.ea.2010`,\n",
    "DistBelief :cite:`Dean.Corrado.Monga.ea.2012`,\n",
    "and Caffe :cite:`Jia.Shelhamer.Donahue.ea.2014`\n",
    "arguably represent the\n",
    "first generation of such models \n",
    "that found widespread adoption.\n",
    "In contrast to earlier (seminal) works like\n",
    "SN2 (Simulateur Neuristique) :cite:`Bottou.Le-Cun.1988`,\n",
    "which provided a Lisp-like programming experience,\n",
    "modern frameworks offer automatic differentiation\n",
    "and the convenience of Python.\n",
    "These frameworks allow us to automate and modularize\n",
    "the repetitive work of implementing gradient-based learning algorithms.\n",
    "\n",
    "In :numref:`sec_linear_scratch`, we relied only on\n",
    "(i) tensors for data storage and linear algebra;\n",
    "and (ii) automatic differentiation for calculating gradients.\n",
    "In practice, because data iterators, loss functions, optimizers,\n",
    "and neural network layers\n",
    "are so common, modern libraries implement these components for us as well.\n",
    "In this section, (**we will show you how to implement\n",
    "the linear regression model**) from :numref:`sec_linear_scratch`\n",
    "(**concisely by using high-level APIs**) of deep learning frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b312d",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "When we implemented linear regression from scratch\n",
    "in :numref:`sec_linear_scratch`,\n",
    "we defined our model parameters explicitly\n",
    "and coded up the calculations to produce output\n",
    "using basic linear algebra operations.\n",
    "You *should* know how to do this.\n",
    "But once your models get more complex,\n",
    "and once you have to do this nearly every day,\n",
    "you will be glad of the assistance.\n",
    "The situation is similar to coding up your own blog from scratch.\n",
    "Doing it once or twice is rewarding and instructive,\n",
    "but you would be a lousy web developer\n",
    "if you spent a month reinventing the wheel.\n",
    "\n",
    "For standard operations,\n",
    "we can [**use a framework's predefined layers,**]\n",
    "which allow us to focus\n",
    "on the layers used to construct the model\n",
    "rather than worrying about their implementation.\n",
    "Recall the architecture of a single-layer network\n",
    "as described in :numref:`fig_single_neuron`.\n",
    "The layer is called *fully connected*,\n",
    "since each of its inputs is connected\n",
    "to each of its outputs\n",
    "by means of a matrix--vector multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cc184",
   "metadata": {},
   "source": [
    "In PyTorch, the fully connected layer is defined in `Linear` and `LazyLinear` classes (available since version 1.8.0). \n",
    "The latter\n",
    "allows users to specify *merely*\n",
    "the output dimension,\n",
    "while the former\n",
    "additionally asks for\n",
    "how many inputs go into this layer.\n",
    "Specifying input shapes is inconvenient and may require nontrivial calculations\n",
    "(such as in convolutional layers).\n",
    "Thus, for simplicity, we will use such \"lazy\" layers\n",
    "whenever we can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb89745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.LazyLinear(1)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b679678",
   "metadata": {},
   "source": [
    "In the `forward` method we just invoke the built-in `__call__` method of the predefined layers to compute the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def forward(self, X):\n",
    "    return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309fab8",
   "metadata": {},
   "source": [
    "## Defining the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5418a93",
   "metadata": {},
   "source": [
    "[**The `MSELoss` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).**]\n",
    "By default, `MSELoss` returns the average loss over examples.\n",
    "It is faster (and easier to use) than implementing our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def loss(self, y_hat, y):\n",
    "    fn = nn.MSELoss()\n",
    "    return fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36175986",
   "metadata": {},
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e7184",
   "metadata": {},
   "source": [
    "Minibatch SGD is a standard tool\n",
    "for optimizing neural networks\n",
    "and thus PyTorch supports it alongside a number of\n",
    "variations on this algorithm in the `optim` module.\n",
    "When we (**instantiate an `SGD` instance,**)\n",
    "we specify the parameters to optimize over,\n",
    "obtainable from our model via `self.parameters()`,\n",
    "and the learning rate (`self.lr`)\n",
    "required by our optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7fac3",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "You might have noticed that expressing our model through\n",
    "high-level APIs of a deep learning framework\n",
    "requires fewer lines of code.\n",
    "We did not have to allocate parameters individually,\n",
    "define our loss function, or implement minibatch SGD.\n",
    "Once we start working with much more complex models,\n",
    "the advantages of the high-level API will grow considerably.\n",
    "\n",
    "Now that we have all the basic pieces in place,\n",
    "[**the training loop itself is the same\n",
    "as the one we implemented from scratch.**]\n",
    "So we just call the `fit` method (introduced in :numref:`oo-design-training`),\n",
    "which relies on the implementation of the `fit_epoch` method\n",
    "in :numref:`sec_linear_scratch`,\n",
    "to train our model.\n",
    "\n",
    "**THE FOLLOWING FOUR LINES ARE KEY TO THE FRAMEWORK - MODEL OBJECT, DATA OBJECT, TRAINING OBJECT and then TRAIN.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb003a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(lr=0.03)\n",
    "data = SyntheticRegressionData(w=torch.tensor([2.0,]), b=4.2)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09fe80",
   "metadata": {},
   "source": [
    "Below, we\n",
    "[**compare the model parameters learned\n",
    "by training on finite data\n",
    "and the actual parameters**]\n",
    "that generated our dataset.\n",
    "To access parameters,\n",
    "we access the weights and bias\n",
    "of the layer that we need.\n",
    "As in our implementation from scratch,\n",
    "note that our estimated parameters\n",
    "are close to their true counterparts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def get_w_b(self):\n",
    "    return (self.net.weight.data, self.net.bias.data)\n",
    "w, b = model.get_w_b()\n",
    "\n",
    "w, b, data.w, data.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd921f2e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This section contains the first\n",
    "implementation of a deep network (in this book)\n",
    "to tap into the conveniences afforded\n",
    "by modern deep learning frameworks,\n",
    "such as MXNet :cite:`Chen.Li.Li.ea.2015`, \n",
    "JAX :cite:`Frostig.Johnson.Leary.2018`, \n",
    "PyTorch :cite:`Paszke.Gross.Massa.ea.2019`, \n",
    "and Tensorflow :cite:`Abadi.Barham.Chen.ea.2016`.\n",
    "We used framework defaults for loading data, defining a layer,\n",
    "a loss function, an optimizer and a training loop.\n",
    "Whenever the framework provides all necessary features,\n",
    "it is generally a good idea to use them,\n",
    "since the library implementations of these components\n",
    "tend to be heavily optimized for performance\n",
    "and properly tested for reliability.\n",
    "At the same time, try not to forget\n",
    "that these modules *can* be implemented directly.\n",
    "This is especially important for aspiring researchers\n",
    "who wish to live on the leading edge of model development,\n",
    "where you will be inventing new components\n",
    "that cannot possibly exist in any current library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad09d6",
   "metadata": {},
   "source": [
    "In PyTorch, the `data` module provides tools for data processing,\n",
    "the `nn` module defines a large number of neural network layers and common loss functions.\n",
    "We can initialize the parameters by replacing their values\n",
    "with methods ending with `_`.\n",
    "Note that we need to specify the input dimensions of the network.\n",
    "While this is trivial for now, it can have significant knock-on effects\n",
    "when we want to design complex networks with many layers.\n",
    "Careful considerations of how to parametrize these networks\n",
    "is needed to allow portability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9ad1a-882e-4e9d-afca-8efed0a2ea10",
   "metadata": {},
   "source": [
    "# Plot sample data, theoretical function and the fitted model function. #\n",
    "\n",
    "********** **DO NOT MODIFY THE CODE BELOW !** **********\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3edb859-06a3-42a8-b28f-592318095c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ********** DO NOT MODIFY THIS CODE ********** #\n",
    "\n",
    "# ********** IT SHOULD WORK WITHOUT CHANGING THIS CODE ********** #\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize']  = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "X = data.X.squeeze(1)\n",
    "y = data.y.squeeze(1)\n",
    "\n",
    "xgrid = torch.linspace(torch.min(X),torch.max(X),100)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[0:100],y[0:100],10,'r')\n",
    "plt.scatter(X[100:200],y[100:200],10,'b')\n",
    "\n",
    "plt.plot(xgrid,data.ground_truth_function(xgrid),'k')\n",
    "plt.plot(xgrid,model(xgrid.unsqueeze(1)).detach(),'g')\n",
    "\n",
    "plt.title('Observed data, true function and model function')\n",
    "plt.legend(['Training Data Sample','Validation Data Sample', 'Ground Truth Function','Model Function'])\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add30a08-6041-4724-b73f-b87ecbd994d2",
   "metadata": {},
   "source": [
    "# Moodle Submission #\n",
    "\n",
    "Once you modify the code and get the output plot looking like the one shown on Moodle - then please submit your complete notebook to Moodle.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54acdd5d-7c8b-4626-862e-4bd6509aaa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
